{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500,
     "referenced_widgets": [
      "fded1108e4a749f591cca34076c2bc64",
      "cd19a5cceda54d1192ff814c9f565b6f",
      "bc6f13dfb79f482cac4b397d2e20ba6e",
      "18a66c189c9f4e6a971134164b990b2f",
      "1150390b7c844b2aadb5b0dca6d4b066",
      "37a220a6b6ca4e8f886e4d7759930f2c",
      "17c64e09310e47dc8fb725ef7669a9cd",
      "d591c9cba3bf4a08a7d58071d9397a7b",
      "3acab9dc9aa94d2190bae15abb267d1d",
      "d2af54fa810342babf5869b85826c0ce",
      "b7945c94a88f476a828daff6441b1488",
      "39e7a232f06e4b3e9322c360d3d24f6a",
      "602ce01a49224cdb99caa81859df6199",
      "77e89478574c4e6faf677ef4de641032",
      "b3174144f90344f0b44beb22de4657e9",
      "fce7b004c9324356b5664e9f0bad5c68",
      "6e9ba8dd2a7e46caad15c6a7cf8ec27a",
      "8917c788a8e34d078d7f8fd88e1cf186",
      "d332f799ef55475880d605768629a87a",
      "ebb86ba4cb3546d3bc13f83e48bcf86e",
      "5ea1e8eeaee44379b3bfc9b696d68b2c",
      "b0df6303d9ea4724b9a48408fd5339fb",
      "e2f3902c9e83439a8b4061965a978921",
      "235fc1d3c5d84b2589490be112ee0705",
      "3d540fbca1b746ba834b4f3a3bf8152a",
      "b2d86914e28c47d381fea80adac39358",
      "8883236c48ab4b448db44422231e8b62",
      "a942abfe709d4b3d9c3f76b5305f6251",
      "c6d7bd07482549b3a601ef7a2a4d1689",
      "5ca662e099db4b6db34cf5d21204607a",
      "5f15a984bbbb487c8f396afe599d86b6",
      "09373578f73f423fa1ca6d4063b0b749",
      "00901df819044ce198f7abd7296c389a",
      "646f4b6761d7419096e64e04ad824199",
      "e5d5e1de20944435ba6f321393ef135c",
      "e59bc4caf8534afd85aca519c8f642d9",
      "8f9fce9ad6934f29a93eadd0730b8973",
      "d0da9da7d51b4be382359ced568ea065",
      "8be504cb05eb4dc683819521e519bb83",
      "cfe4caa7636d40aab75fda18ef8722f2",
      "16719f5c2b4c4185bd26edc0ad810234",
      "32995ed96a044c89ad414781602c0bc1",
      "bae009ce91294522a1e57ffef53e4124",
      "e63c5466ff83449d8ca3b88f5b5589d4",
      "288ad85fc0d84143841cebb302390be6",
      "cd1db489570b457f9244c9c785b1b337",
      "a6120bebb9fd4abf9e16709b75b7aa47",
      "0ad107d1d6264390949a5b7996dbe047",
      "87734fe422974735abf6084addf05dd9",
      "83438bc4be44497687050f4741c90b0b",
      "58de95f10cf44ebe92327a4e72f69f4a",
      "226ca85027674e40b5f7d9df2fe60147",
      "c8e17eb9cd214d07941a15bf89e45a1b",
      "eae92f6c67de4e7bbe5c9d89e5ef06ea",
      "a1e156cd12a447878a4fa1657a61f6e0",
      "8328f2eb86774b1d86899becbbca7d20",
      "5bced43ebf794bfcb74d35dd04266b62",
      "2beac96836914be1a09b97c8bd29f131",
      "2f9fcec4a3e24eeaa4346c0e86fc8bbb",
      "d387a8bc1f5b4063bbdc2eb1152d1ccb",
      "2f34c7579e3e43e4bc159b7099ca132c",
      "b5f4a776e38848f2a4cdef273baa14e8",
      "f1e5cb32ac46449eb4d06c4c25c6d1c5",
      "24c07bd792db48db83c8b445da6c0fe5",
      "189c0edde0144778832ec3bee74e1685",
      "cba14f7f290f4e0f993103a7647e4d18",
      "04cd4bc91ddc4096a9422d4d148d6113",
      "41e0bbac91924dc38a2a118bc1d87a2e",
      "e62d8ffd961443b0b290b075afa48a84",
      "0c7ee7e8bb754d91beffe5ed0b2b6fbe",
      "04ac83fda6df4b38abcbecd63770b113",
      "285ce5de55da4c6090aee12bc9793d08",
      "08f3446477e1402d94cf46f816b278ed",
      "80c2655c1b2c4875a68895d05dfbbf1f",
      "52a13ecfe7d940c99a582057773e412d",
      "6a7a4a9ebb3e40ac85b9e4fa7591dac6",
      "e17be187778d4c80a96dae12d1d6102e",
      "6ad7b5be244747458ed7e30e84db031a",
      "5edd460afd4541c596aa59445481c216",
      "1acb824d516a474c9985e9574275b861",
      "eb36acef86ee4ecdbe5b1446523bf2e2",
      "467c0f50288944f2bfce0208989633e4",
      "249d1fe6fd7f4024b304f3eb8ca6dbf8",
      "f3db9593d9ca4ca6b14ce356d7a789be",
      "73dd611bd7264e5c97ce4730f599c72d",
      "dd3674e130264a79bf05f8437910079e",
      "899a2d9490a144608f393e5f4b72a7c4",
      "c8e1ed9a0d3b45ff8fc30f11e45249d9",
      "f036ff8c5c7d422f8815a7fb6a410f5a",
      "84f2e5377aee4555ae2a347daf78e873",
      "3a2a718c09144a2598b3b5a030018f2c",
      "04eff9cc5cbc4a3c8863163c3c1ea315",
      "3619b5b6bb8f4cc885e3858b7240a5a1",
      "6e24c9bd333a4b3aa0904372dcc5e5ba",
      "a3e23f12ccdf4fa6a83a82c1c5f5efa3",
      "c281dff9f75c45ce8341d1955a16883f",
      "aa307d0db9674f6098f402360eaa55f7",
      "28c9ba9372d14afb976bf6bc8aa07ede",
      "8487788f5b94479186048fd171284660",
      "db155f8250944f1581c09b4021fb49ad",
      "d51b04aeea5d493ab7715264d9086507",
      "88f7c14263434395aa7b37333d8f9392",
      "bb8d40d12058400dbcdb162c2d5cd3e1",
      "d16ca99fdfed4b20917c68cc8ef86b2b",
      "2b2d91afd7f64d33b72e48be2955465e",
      "779bc66a2c68427ca415d40f726809aa",
      "c522eb8b24bc48d985430061183b7a51",
      "fff537e10ebd4a409c3f495c11c893d9",
      "4d8b209ae1fe4cc99767eb6f842cd8e2",
      "e0b810b0e5c44f029d874dd9ec3808b0"
     ]
    },
    "id": "1nZVY8xBpQDc",
    "outputId": "800e9c87-3975-4201-91f0-2171b0977ecb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fded1108e4a749f591cca34076c2bc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e7a232f06e4b3e9322c360d3d24f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f3902c9e83439a8b4061965a978921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646f4b6761d7419096e64e04ad824199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288ad85fc0d84143841cebb302390be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8328f2eb86774b1d86899becbbca7d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/735M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cd4bc91ddc4096a9422d4d148d6113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad7b5be244747458ed7e30e84db031a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f036ff8c5c7d422f8815a7fb6a410f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db155f8250944f1581c09b4021fb49ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from tokenizers.decoders import WordPiece\n",
    "\n",
    "# Load model\n",
    "model_name = \"avichr/heBERT_NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "oracle = pipeline('ner', model='dicta-il/dictabert-ner', aggregation_strategy='simple')\n",
    "oracle.tokenizer.backend_tokenizer.decoder = WordPiece()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x0hxF71OrlR0"
   },
   "outputs": [],
   "source": [
    "# Load uploaded file\n",
    "df = pd.read_csv(\"/content/AlephBERT_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8KqUgvQ6dKyq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define intent templates\n",
    "intent_templates = {\n",
    "    \"transport_query\": \"את יכולה למצוא לי מסלול {origin} {destination} ב{mode}?\",\n",
    "    \"alarm_set\": \"את יכולה לכוון שעון מעורר לשעה {time} {period}\",\n",
    "    \"call_contact\": \"את יכולה לחייג ל{contact_name}?\",\n",
    "    \"send_message\": \"את יכולה לשלוח הודעה ל{contact_name} {message}\",\n",
    "    \"calendar_set\": \"את יכולה ליצור לי פגישה ביומן ל{date} ב{time} בשם {meeting_title}?\",\n",
    "    \"camera_query\": \"אני רוצה לצלם {type}\",\n",
    "    \"lists_createoradd\": \"תיצרי לי פתק חדש שיהיה כתוב בו {items}\",\n",
    "    \"weather_query\": \"מה מזג האוויר {date} {location}?\",\n",
    "    \"iot_wemo_on\": \"אני רוצה להדליק {device}\",\n",
    "    \"query\": \"את יכולה לבדוק לי {search string}\"\n",
    "}\n",
    "\n",
    "\n",
    "# Extract keys from templates\n",
    "intent_keys = {\n",
    "    intent: re.findall(r\"{(.*?)}\", template)\n",
    "    for intent, template in intent_templates.items()\n",
    "}\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def extract_entities(ner_result, label):\n",
    "    return [entity[\"word\"] for entity in ner_result if label in entity[\"entity_group\"]]\n",
    "\n",
    "def extract_entity(ner_result, label):\n",
    "    entities = extract_entities(ner_result, label)\n",
    "    return entities[0] if entities else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pZ1Fu-T2E_m",
    "outputId": "018d7bdd-9b05-421d-f058-8fb1dfe9f846"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "def extract_destination_fallback(transcript):\n",
    "    # טיפול מיוחד בביטויים שמתחילים ב\"תפתחי לי ב[maps|מפות] ...\"\n",
    "    match = re.search(r\"תפתחי(?:\\s+לי)?\\s+ב(?:maps|מפות)?\\s+(.*)\", transcript)\n",
    "    if match:\n",
    "        possible_address = match.group(1).strip()\n",
    "        # נסה למצוא כתובת בתוך מה שנשאר\n",
    "        addr_match = re.search(r\"(שדרות|רחוב|דרך)\\s+[א-ת׳\\\"״]+\\s*\\d*\\s*(?:ב[א-ת\\s]+)?\", possible_address)\n",
    "        if addr_match:\n",
    "            return addr_match.group().strip()\n",
    "        return possible_address\n",
    "\n",
    "    # טיפול בביטוי \"עם הכתובת\"\n",
    "    match = re.search(r\"(?:לפתוח\\s+)?(?:גוגל\\s+)?מפס\\s+עם\\s+הכתובת\\s+(.+)\", transcript)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # ניסיון לזהות כתובת בפורמט סטנדרטי\n",
    "    street_pattern = re.search(r\"(שדרות|רחוב|דרך)\\s+[א-ת\\\"׳״]+\\s*\\d*\\s*ב[א-ת\\\"׳״\\s]+\", transcript)\n",
    "    if street_pattern:\n",
    "        return street_pattern.group().strip()\n",
    "\n",
    "    # fallback רגיל\n",
    "    ignored_starters = {\n",
    "        \"לקבל\", \"לקבל הסבר\", \"לקבל מידע\", \"לקבל עזרה\",\n",
    "        \"לנווט\", \"לכוון\", \"להגיע\", \"ללכת\", \"לנסוע\",\n",
    "        \"תעזור\", \"תעזרי\", \"תכווני\", \"תכוון\", \"לפתוח\", \"גוגל\", \"מפס\",\n",
    "        \"maps\", \"עם\", \"הכתובת\", \"תפתחי\", \"במפות\", \"ב־maps\", \"לי\"\n",
    "    }\n",
    "\n",
    "    stopwords = {\n",
    "        \"ש\", \"אם\", \"אז\", \"אבל\", \"ואם\", \"או\", \"על\", \"של\", \"עם\", \"שבו\", \"ב\", \"לב\", \"ול\",\n",
    "        \"האם\", \"איך\", \"תגידי\", \"תגיד\", \"תגידו\", \"תוכלי\", \"תוכל\", \"לעזור\", \"לי\", \"בבקשה\",\n",
    "        \"אתה\", \"אני\", \"אולי\", \"תעזרי\", \"תעזור\", \"תכוון\", \"תכווני\", \"מפה\", \"צריכה\", \"צריך\", \"סימון\"\n",
    "    }\n",
    "\n",
    "    for phrase in ignored_starters:\n",
    "        transcript = transcript.replace(phrase, \"\")\n",
    "\n",
    "    words = transcript.strip().split()\n",
    "    bad_starts = {\"מפה\", \"תמונה\", \"סימון\", \"תצוגה\"}\n",
    "    while words and words[0] in bad_starts:\n",
    "        words = words[1:]\n",
    "\n",
    "    collecting = False\n",
    "    dest_words = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        stripped = word.strip(\".,!?\")\n",
    "        if not collecting and (stripped.startswith(\"ל\") and len(stripped) > 3):\n",
    "            collecting = True\n",
    "            dest_words = [stripped]\n",
    "            continue\n",
    "        if collecting:\n",
    "            if stripped in stopwords:\n",
    "                break\n",
    "            dest_words.append(stripped)\n",
    "\n",
    "    if not dest_words and len(words) >= 2:\n",
    "        candidate = []\n",
    "        for word in words:\n",
    "            w = word.strip(\".,!?\")\n",
    "            if w in stopwords:\n",
    "                break\n",
    "            candidate.append(w)\n",
    "        dest_words = candidate\n",
    "\n",
    "    while dest_words and dest_words[-1] in {\"ב\", \"ל\", \"של\", \"על\", \"מפה\"}:\n",
    "        dest_words.pop()\n",
    "\n",
    "    return \" \".join(dest_words)\n",
    "\n",
    "\n",
    "def clean_destination_of_mode(destination, mode):\n",
    "    \"\"\"Remove mode word from the end of the destination, if it appears.\"\"\"\n",
    "    if destination.endswith(mode):\n",
    "        return destination[:-len(mode)].strip()\n",
    "    return destination\n",
    "\n",
    "def clean_destination_suffix(destination):\n",
    "    # הסר תווים חריגים מסוף הכתובת\n",
    "    destination = destination.strip(\".,!? \").strip()\n",
    "\n",
    "    # הסר מילה בודדת לא לגיטימית בסוף (למשל \"ב\", \"ל\", \"של\", \"על\")\n",
    "    if re.search(r\"\\b(ב|ל|של|על)$\", destination):\n",
    "        destination = re.sub(r\"\\b(ב|ל|של|על)$\", \"\", destination).strip()\n",
    "    return destination\n",
    "\n",
    "\n",
    "\n",
    "def build_action_json(transcript, ner_result, intent):\n",
    "    intent = intent.strip()\n",
    "    keys = intent_keys.get(intent, [])\n",
    "    result = {\"intent\": intent}\n",
    "\n",
    "    # Extract entities\n",
    "    full_name = extract_entity(ner_result, \"PER\")\n",
    "\n",
    "    if \"contact_name\" in keys and full_name:\n",
    "        # ניקוי תווים לא רצויים\n",
    "        full_name = full_name.replace(\"##\", \"\").strip()\n",
    "\n",
    "        # אם מתחיל ב\"ל\" ונשמע כמו שם (למשל: ליוני, לדנה) – הסר את \"ל\"\n",
    "        if full_name.startswith(\"ל\") and len(full_name) > 2:\n",
    "            full_name = full_name[1:]\n",
    "\n",
    "        result[\"contact_name\"] = full_name\n",
    "\n",
    "\n",
    "    location = extract_entity(ner_result, \"LOC\")\n",
    "    date = extract_entity(ner_result, \"DATE\")\n",
    "    time = extract_entity(ner_result, \"TIME\")\n",
    "    locs = extract_entities(ner_result, \"LOC\")\n",
    "    msg = transcript.split(\":\")[-1] if \":\" in transcript else transcript.split(full_name)[-1].strip() if full_name and full_name in transcript else None\n",
    "\n",
    "\n",
    "    for key in keys:\n",
    "        if intent == \"transport_query\":\n",
    "            result[\"mode\"] = \"הליכה\"\n",
    "            result[\"destination\"] = \"\"\n",
    "            result[\"origin\"] = \"מהמיקום שלי\"\n",
    "\n",
    "            modes = {\n",
    "                (\"רכבת\", \"אוטובוס\"): \"תחבורה ציבורית\",\n",
    "                (\"רגל\", \"הלכ\"): \"הליכה\",\n",
    "                (\"רכב\", \"ברכב\", \"אוטו\", \"נהיגה\", \"לנסוע\"): \"רכב\"\n",
    "            }\n",
    "\n",
    "            for mode in modes:\n",
    "                for transport in mode:\n",
    "                    regex = rf\"{transport}\"\n",
    "                    if re.search(regex, transcript):\n",
    "                        result[\"mode\"] = modes[mode]\n",
    "                        continue\n",
    "\n",
    "            ner_result = oracle(transcript)\n",
    "\n",
    "            allowed_ents = [\"GPE\", \"FAC\", \"PER\", \"CARDINAL\", \"NUMBER\", \"LOC\"]\n",
    "            to_regex = r\"^ל\"\n",
    "            from_regex = r\"^מ\"\n",
    "\n",
    "            prev_state = None\n",
    "            destination_words = []\n",
    "            origin_words = []\n",
    "\n",
    "            # מילים שמתחילות ב\"ל\" אבל אינן כתובות, אלא פעלים כלליים\n",
    "            invalid_to_words = {\n",
    "                \"להגיע\", \"להראות\", \"לנסוע\", \"לפתוח\", \"לכוון\", \"לחפש\", \"לבדוק\",\n",
    "                \"לשאול\", \"לראות\", \"לשמור\", \"להתקשר\", \"לשלוח\", \"לנווט\"\n",
    "            }\n",
    "\n",
    "            for res in ner_result:\n",
    "                extracted_word = transcript[res['start']:min(res['end'] + 1, len(transcript))].strip()\n",
    "                entity = res['entity_group']\n",
    "\n",
    "                if entity in allowed_ents:\n",
    "                    if re.search(to_regex, extracted_word) and not destination_words:\n",
    "                        if extracted_word in invalid_to_words:\n",
    "                            continue  # התעלם מהמילה הזו — זה פועל, לא יעד\n",
    "                        destination_words.append(extracted_word)\n",
    "                        prev_state = \"destination\"\n",
    "                    elif re.search(from_regex, extracted_word) and not origin_words:\n",
    "                        origin_words.append(extracted_word)\n",
    "                        prev_state = \"origin\"\n",
    "                    elif prev_state == \"destination\":\n",
    "                        destination_words.append(extracted_word)\n",
    "                    elif prev_state == \"origin\":\n",
    "                        origin_words.append(extracted_word)\n",
    "                else:\n",
    "                    prev_state = None\n",
    "\n",
    "\n",
    "            if destination_words:\n",
    "                result[\"destination\"] = \" \".join(destination_words)\n",
    "                result[\"destination\"] = clean_destination_of_mode(result[\"destination\"], result[\"mode\"])\n",
    "\n",
    "                result[\"destination\"] = clean_destination_suffix(result[\"destination\"])\n",
    "\n",
    "            if origin_words:\n",
    "                result[\"origin\"] = \" \".join(origin_words) or \"מהמיקום שלי\"\n",
    "\n",
    "            # Fallback destination if NER failed\n",
    "            if not result[\"destination\"]:\n",
    "                fallback_address = extract_destination_fallback(transcript)\n",
    "                if fallback_address:\n",
    "                    result[\"destination\"] = clean_destination_of_mode(fallback_address, result[\"mode\"])\n",
    "                    result[\"destination\"] = clean_destination_suffix(result[\"destination\"])\n",
    "\n",
    "\n",
    "        elif key == \"period\":\n",
    "            if any(p in transcript for p in [\"בבוקר\", \"בוקר\"]):\n",
    "                result[key] = \"בבוקר\"\n",
    "            elif any(p in transcript for p in [\"בערב\", \"לערב\", \"ערב\"]):\n",
    "                result[key] = \"בערב\"\n",
    "            elif any(p in transcript for p in [\"בלילה\", \"ללילה\"]):\n",
    "                result[key] = \"בלילה\"\n",
    "            elif \"אחר הצהריים\" in transcript or \"בצהריים\" in transcript:\n",
    "                result[key] = \"בצהריים\"\n",
    "            elif \"מחר\" in transcript:\n",
    "                result[key] = \"מחר\"\n",
    "            elif \"היום\" in transcript:\n",
    "                result[key] = \"היום\"\n",
    "            elif time:\n",
    "                try:\n",
    "                    hour = int(time.split(\":\")[0])\n",
    "                    if 5 <= hour < 12:\n",
    "                        result[key] = \"בבוקר\"\n",
    "                    elif 12 <= hour < 17:\n",
    "                        result[key] = \"בצהריים\"\n",
    "                    else:\n",
    "                        result[key] = \"בערב\"\n",
    "                except:\n",
    "                    result[key] = None\n",
    "            else:\n",
    "                result[key] = None\n",
    "\n",
    "\n",
    "        elif key == \"contact_name\":\n",
    "            if full_name:\n",
    "                clean_name = full_name.strip()\n",
    "                # טיפול בכפילות של מילים עוקבות (כמו \"אנה אנה לוי\")\n",
    "                words = clean_name.split()\n",
    "                deduped = []\n",
    "                for i, word in enumerate(words):\n",
    "                    if i == 0 or word != words[i - 1]:\n",
    "                        deduped.append(word)\n",
    "                result[key] = \" \".join(deduped)\n",
    "            else:\n",
    "                match = re.search(r\"ל([א-ת]+(?: [א-ת]+)?)\", transcript)\n",
    "                if match:\n",
    "                    name = match.group(1).strip()\n",
    "                    result[key] = name\n",
    "                else:\n",
    "                    result[key] = None\n",
    "\n",
    "\n",
    "        elif key == \"message\":\n",
    "            msg = None\n",
    "\n",
    "            # אוסף ביטויים טיפוסיים של הודעות\n",
    "            common_phrases = [\n",
    "                r\"תתקשר(י|י)? אליי כשתתפנה(?: בבקשה)?\",\n",
    "                r\"תתקשר אלי כשתתפנה(?: בבקשה)?\",\n",
    "                r\"שיתקשר אליי כשהוא מתפנה\",\n",
    "                r\"שיתקשר אלי כשהוא מתפנה\",\n",
    "                r\"שיתקשר אליי כשיהיה פנוי\",\n",
    "                r\"שיתקשר אלי כשיהיה פנוי\",\n",
    "                r\"ש?הוא יתקשר אליי כשהוא מתפנה\",\n",
    "                r\"ש?הוא יתקשר אלי כשהוא מתפנה\",\n",
    "                r\"ש?הוא יתקשר אליי כשיהיה פנוי\",\n",
    "                r\"ש?הוא יתקשר אלי כשיהיה פנוי\"\n",
    "            ]\n",
    "\n",
    "            for phrase in common_phrases:\n",
    "                match = re.search(phrase, transcript)\n",
    "                if match:\n",
    "                    msg = match.group().strip()\n",
    "                    break\n",
    "\n",
    "            if not msg:\n",
    "                # fallback: טקסט אחרי 'באס אם אס'\n",
    "                match = re.search(r\"באס[ -]?א?ם[ -]?א?ס\\s+(.*)\", transcript)\n",
    "                if match:\n",
    "                    msg = match.group(1).strip()\n",
    "\n",
    "            # ניקוי מילים מיותרות\n",
    "            if msg:\n",
    "                garbage_patterns = [\n",
    "                    r\"ה?ה?ודעת( ה)?טקסט( הבאה)?(?:\\s+בבקשה)?(?:\\s+תשלחי\\s+לו)?\",\n",
    "                    r\"באס ?אם ?אס\",\n",
    "                    r\"סמס\", r\"SMS\", r\"ב[- ]?SMS\",\n",
    "                    r\"\\bבבקשה\\b\", r\"\\bאת\\b\", r\"\\bהבאה\\b\", r\"\\bב\\b\",\n",
    "                    r\"תשלחי לו\", r\"אנא שלחי\", r\"שלחי\",\n",
    "                    r\"לישראל זילברמן\", r\"לאלי זילברמן\"\n",
    "                ]\n",
    "\n",
    "                cleaned_msg = msg\n",
    "                for pattern in garbage_patterns:\n",
    "                    cleaned_msg = re.sub(pattern, \"\", cleaned_msg, flags=re.IGNORECASE)\n",
    "\n",
    "                cleaned_msg = re.sub(r\"\\s{2,}\", \" \", cleaned_msg).strip()\n",
    "                result[key] = cleaned_msg if cleaned_msg else None\n",
    "            else:\n",
    "                result[key] = None\n",
    "\n",
    "\n",
    "\n",
    "        elif key == \"meeting_title\":\n",
    "            title = None\n",
    "\n",
    "            # חיפוש ביטויים שמסמנים שם פגישה\n",
    "            patterns = [\n",
    "                r\"(?:בשם|שמה הפגישה תהיה|ותקראי ל[הו]?|תני שם לפגישה|תקראי ל[הו]?)\\s+(.+?)(?=\\s+(?:ש[הי]וא|פגישה|עם|בתאריך|בשעה|ברביעי|בחמישי|ב[\\w]+)|$)\",\n",
    "                r\"פגישה בשם\\s+(.+?)(?=\\s+(?:ש[הי]וא|עם|בתאריך|בשעה|ברביעי|בחמישי|ב[\\w]+)|$)\"\n",
    "            ]\n",
    "\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, transcript)\n",
    "                if match:\n",
    "                    title = match.group(1).strip(\" .,\")\n",
    "                    break\n",
    "\n",
    "            result[key] = title if title else \"פגישה\"\n",
    "\n",
    "\n",
    "\n",
    "        elif key == \"date\":\n",
    "            if intent == \"weather_query\":\n",
    "                result[key] = date or \"היום\"\n",
    "            else:\n",
    "                date_value = None\n",
    "\n",
    "                # שלב 1: תאריך במספרים (למשל 04/06/2025)\n",
    "                date_match = re.search(r\"\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{2,4})\\b\", transcript)\n",
    "                if date_match:\n",
    "                    day, month, year = date_match.groups()\n",
    "                    date_value = f\"{int(day):02}/{int(month):02}/{year}\"\n",
    "\n",
    "                # שלב 2: תאריך מילולי (למשל \"ברביעי לשישי\", \"בראשון ביולי\")\n",
    "                if not date_value:\n",
    "                    verbal_match = re.search(\n",
    "                        r\"(?:בתאריך\\s*)?(?:ב)?(ראשון|שני|שלישי|רביעי|חמישי|שישי|שבת)\\s*(?:ל|ב)([א-ת]+|\\d{1,2})(?:\\s*(\\d{2,4}))?\",\n",
    "                        transcript\n",
    "                    )\n",
    "                    if verbal_match:\n",
    "                        weekday, month, year = verbal_match.groups()\n",
    "\n",
    "                        # מיפוי של שמות חודשים עבריים ממוספרים למילוליים\n",
    "                        hebrew_month_map = {\n",
    "                            \"ראשון\": \"ינואר\",\n",
    "                            \"שני\": \"פברואר\",\n",
    "                            \"שלישי\": \"מרץ\",\n",
    "                            \"רביעי\": \"אפריל\",\n",
    "                            \"חמישי\": \"מאי\",\n",
    "                            \"שישי\": \"יוני\",\n",
    "                            \"שביעי\": \"יולי\",\n",
    "                            \"שמיני\": \"אוגוסט\",\n",
    "                            \"תשיעי\": \"ספטמבר\",\n",
    "                            \"עשירי\": \"אוקטובר\",\n",
    "                            \"אחד עשר\": \"נובמבר\",\n",
    "                            \"שתים עשרה\": \"דצמבר\"\n",
    "                        }\n",
    "\n",
    "                        # ניקוי הקידומת (\"ל\", \"ב\") מהמילה כדי לבדוק במפה\n",
    "                        month_clean = re.sub(r\"^(ל|ב)\", \"\", month)\n",
    "                        month_normalized = hebrew_month_map.get(month_clean, month_clean)\n",
    "\n",
    "                        parts = [weekday]\n",
    "                        parts.append(\"ל\" + month_normalized)\n",
    "                        if year:\n",
    "                            parts.append(year)\n",
    "                        date_value = \" \".join(parts).strip()\n",
    "\n",
    "                # שלב 3: Fallback למידע מ-NER אם יש משהו שימושי\n",
    "                if not date_value and date and date.strip() not in {\"בתאריך\", \"לתאריך\"}:\n",
    "                    date_value = date.strip()\n",
    "\n",
    "                result[key] = date_value or \"היום\"\n",
    "\n",
    "        elif key == \"time\":\n",
    "            time_candidate = None\n",
    "\n",
    "            # שלב 1: פורמט רגיל - 6:48, 06.30 וכו'\n",
    "            time_match = re.search(r\"(\\d{1,2})[:٫.](\\d{2})\", transcript)\n",
    "            if time_match:\n",
    "                result[key] = f\"{int(time_match.group(1)):02}:{time_match.group(2)}\"\n",
    "                continue\n",
    "\n",
    "            # # שלב 2: לשעה / בשעה\n",
    "            # word_time_match = re.search(r\"(?:לשעה|בשעה)\\s+([א-ת\\s]+)\", transcript)\n",
    "            # if word_time_match:\n",
    "            #     raw_time = word_time_match.group(1).strip()\n",
    "\n",
    "            #     # חיתוך במילה שמתחילה ב\"ל\" ולא נראית כמו חלק משעה (למשל: \"לדוקטור\")\n",
    "            #     tokens = raw_time.split()\n",
    "            #     time_tokens = []\n",
    "            #     for token in tokens:\n",
    "            #         if re.match(r\"^ל(?!שעה)\", token) or token in {\"בבקשה\", \"תודה\"}:\n",
    "            #             break\n",
    "            #         time_tokens.append(token)\n",
    "\n",
    "            #     time_candidate = \" \".join(time_tokens).strip()\n",
    "\n",
    "            # שלב 2: לשעה / בשעה\n",
    "            word_time_match = re.search(r\"(?:לשעה|בשעה)\\s+([א-ת\\s]+)\", transcript)\n",
    "            if word_time_match:\n",
    "                raw_time = word_time_match.group(1).strip()\n",
    "\n",
    "                # עצירה במילים שמבשרות על ישויות אחרות (contact, title וכו')\n",
    "                stop_words = {\n",
    "                    \"בשם\", \"תקראי\", \"תקרא\", \"תור\", \"עם\", \"לדוקטור\", \"לד״ר\", \"פגישה\", \"לרופא\", \"הרופא\"\n",
    "                }\n",
    "\n",
    "                tokens = raw_time.split()\n",
    "                time_tokens = []\n",
    "                for token in tokens:\n",
    "                    if token in stop_words or token.startswith(\"ל\") and token not in {\"לחמש\", \"לאחת\", \"לשתיים\"}:\n",
    "                        break\n",
    "                    time_tokens.append(token)\n",
    "\n",
    "                time_candidate = \" \".join(time_tokens).strip()\n",
    "\n",
    "\n",
    "\n",
    "            # שלב 3: אם intent == alarm_set, חפש אחרי \"ל\"/\"ב\" ודלג על \"בקשה\"\n",
    "            elif intent == \"alarm_set\":\n",
    "                words = transcript.split()\n",
    "                for i, word in enumerate(words):\n",
    "                    if word in {\"ב\", \"ל\"} and i + 1 < len(words):\n",
    "                        next_index = i + 1\n",
    "                        if words[next_index] == \"בקשה\":\n",
    "                            next_index += 1\n",
    "                        if next_index < len(words):\n",
    "                            time_candidate = \" \".join(words[next_index:])\n",
    "                            break\n",
    "\n",
    "            # שלב 4: fallback – חיפוש רצף של מילים שמזכירות מספרים\n",
    "            if not time_candidate:\n",
    "                matches = re.findall(\n",
    "                    r\"(שבע|שש|שמונה|חמש|ארבע|תשע|עשר|עשרים|שלושים|חמישים|ארבעים)\"\n",
    "                    r\"(?:\\s+(אחת|שתיים|שלוש|שש|שבע|שמונה|תשע|עשר|עשרים|שלושים|חמישים|ארבעים)*)?\",\n",
    "                    transcript\n",
    "                )\n",
    "                if matches:\n",
    "                    time_candidate = \" \".join([m[0] + (\" \" + m[1] if m[1] else \"\") for m in matches])\n",
    "\n",
    "            # שלב 5: זיהוי ביטויים כמו \"שלוש ורבע\", \"שתיים וחצי\", \"אחת בדיוק\"\n",
    "            if not time_candidate:\n",
    "                time_words_match = re.search(\n",
    "                    r\"(?:שעה\\s+)?(אחת|שתיים|שלוש|ארבע|חמש|שש|שבע|שמונה|תשע|עשר)\"\n",
    "                    r\"(?:\\s+(ורבע|וחצי|בדיוק))?\",\n",
    "                    transcript\n",
    "                )\n",
    "                if time_words_match:\n",
    "                    time_candidate = \" \".join([g for g in time_words_match.groups() if g])\n",
    "\n",
    "\n",
    "            # ניקוי מילים מיותרות\n",
    "            if time_candidate:\n",
    "                # ניקוי ביטויים שלמים קודם\n",
    "                phrases_to_remove = [\n",
    "                    \"תודה מראש\", \"תודה רבה\", \"אני רוצה\", \"אני צריכה\", \"אשמח אם\", \"אפשר לכוון\"\n",
    "                ]\n",
    "                for phrase in phrases_to_remove:\n",
    "                    time_candidate = time_candidate.replace(phrase, \"\")\n",
    "\n",
    "                # ניקוי מילים בודדות אחר כך\n",
    "                garbage = [\n",
    "                    \"תודה\", \"מראש\", \"בבקשה\", \"בכיף\",\n",
    "                    \"תוכלי\", \"תוכל\", \"תכווני\", \"לכוון\", \"לכיוון\",\n",
    "                    \"את\", \"אני\", \"מוכנה\", \"מעורר\", \"שעון\", \"השעון\", \"השעה\"\n",
    "                ]\n",
    "                for word in garbage:\n",
    "                    time_candidate = re.sub(rf\"\\b{word}\\b\", \"\", time_candidate)\n",
    "\n",
    "                # ניקוי רווחים\n",
    "                time_candidate = re.sub(r\"\\s{2,}\", \" \", time_candidate).strip()\n",
    "\n",
    "                if len(time_candidate.split()) >= 1:\n",
    "                    result[key] = time_candidate\n",
    "                else:\n",
    "                    result[key] = None\n",
    "            else:\n",
    "                result[key] = None\n",
    "\n",
    "\n",
    "        elif key == \"items\":\n",
    "            raw_items = transcript\n",
    "\n",
    "            # מילות מפתח שמסמנות את תחילת רשימת הפריטים\n",
    "            item_starters = {\"חלב\", \"ביצים\", \"דגים\", \"מלח\", \"שמן\", \"לחם\", \"עגבניות\", \"מים\", \"שוקולד\", \"אורז\", \"פסטה\"}\n",
    "\n",
    "            # חפש את מיקום ההתחלה של מילה כזו\n",
    "            words = raw_items.split()\n",
    "            start_index = next((i for i, word in enumerate(words) if word.strip(\" ,.\") in item_starters), None)\n",
    "\n",
    "            if start_index is not None:\n",
    "                items_raw = \" \".join(words[start_index:])\n",
    "            else:\n",
    "                # fallback אם לא מצאנו מילה ברורה, נשתמש בגרסה הישנה עם סינון\n",
    "                items_raw = raw_items\n",
    "\n",
    "            # סינון ביטויים כלליים\n",
    "            garbage_phrases = [\n",
    "                \"אני רוצה\", \"אני צריכה\", \"אני צריך\", \"תוסיפי לי\", \"תוסיפי\", \"תכיני לי\",\n",
    "                \"בבקשה\", \"רשימת קניות\", \"רשימה\", \"פתק\", \"הפתק\", \"של\", \"שיהיה כתוב בו\",\n",
    "                \"בפתק\", \"הפתק יקרא\", \"יהיה רשום\", \"שורה למטה\", \"רשום בו\", \"כתוב בו\",\n",
    "                \"את יכולה\", \"לפתוח לי\", \"תפתחי לי\", \"תיצרי לי\", \"תכיני\", \"לי\", \"לי ל\",\n",
    "                \"בפתקים\", \"לפתקים\", \"הקניות\", \"הפתק\", \"פתק חדש\"\n",
    "            ]\n",
    "\n",
    "            for phrase in sorted(garbage_phrases, key=len, reverse=True):\n",
    "                items_raw = items_raw.replace(phrase, \"\")\n",
    "\n",
    "            # ניקוי כפילויות\n",
    "            tokens = items_raw.strip().split()\n",
    "            cleaned_tokens = [t.strip(\",. \") for t in tokens if len(t) > 1]\n",
    "            cleaned = \" \".join(cleaned_tokens)\n",
    "\n",
    "            # פיצול לרשימה והסרת כפילויות\n",
    "            parts = re.split(r\"\\s+וגם\\s+|\\s+ו\\s+|,|\\s+ו(?=\\S)\", cleaned)\n",
    "            unique_cleaned = list(dict.fromkeys([p.strip(\" ,.-\") for p in parts if len(p.strip(\" ,.-\")) > 1]))\n",
    "\n",
    "            # בניית טקסט סופי\n",
    "            if len(unique_cleaned) > 1:\n",
    "                result[key] = \", \".join(unique_cleaned[:-1]) + \" ו\" + unique_cleaned[-1]\n",
    "            elif unique_cleaned:\n",
    "                result[key] = unique_cleaned[0]\n",
    "            else:\n",
    "                result[key] = None\n",
    "\n",
    "\n",
    "\n",
    "        elif key == \"type\":\n",
    "            if \"סלפי\" in transcript or re.search(r\"\\bה?מצלמה( הקדמית)?\\b\", transcript):\n",
    "                result[key] = \"סלפי\"\n",
    "            else:\n",
    "                result[key] = \"תמונה\"\n",
    "\n",
    "        elif key == \"location\":\n",
    "            result[key] = location or (\"תל אביב\" if \"תל אביב\" in transcript else None)\n",
    "\n",
    "        elif key == \"device\":\n",
    "            known_devices = [\"פנס\", \"מצלמה\", \"מצב טיסה\", \"wifi\", \"בלוטות\", \"טעינה\"]\n",
    "            for dev in known_devices:\n",
    "                if dev in transcript:\n",
    "                    result[key] = dev\n",
    "                    break\n",
    "            else:\n",
    "                result[key] = \"התקן\"\n",
    "\n",
    "\n",
    "        # elif key == \"search string\":\n",
    "        #     raw = transcript\n",
    "\n",
    "        #     # מחיקה של ביטויים כלליים ופסולת שפתית\n",
    "        #     garbage_phrases = [\n",
    "        #         \"אני רוצה\", \"אני מעוניינת\",\"מבקשת\", \"אני מבקשת\", \"הייתי רוצה\", \"בבקשה\",\n",
    "        #         \"תודה מראש\", \"תודה רבה\",\"להיות\", \"תודה\", \"לשאול\", \"לדעת\", \"לבדוק\",\n",
    "        #         \"שם\", \"מהו\", \"מה הוא\", \"מהו שם\", \"של ה\", \"תגידי\", \"תגיד\", \"תגידו\"\n",
    "        #     ]\n",
    "        #     for phrase in garbage_phrases:\n",
    "        #         raw = raw.replace(phrase, \"\")\n",
    "\n",
    "        #     # הסרת מילים בודדות מיותרות כמו \"אני\" אם נשארה\n",
    "        #     garbage_words = [\"אני\", \"תודה\", \"להיות\"]\n",
    "        #     for word in garbage_words:\n",
    "        #         raw = re.sub(rf\"\\b{word}\\b\", \"\", raw)\n",
    "\n",
    "        #     # ניקוי רווחים כפולים והוספה של סימן שאלה בסוף\n",
    "        #     cleaned = re.sub(r\"\\s{2,}\", \" \", raw).strip()\n",
    "        #     if not cleaned.endswith(\"?\"):\n",
    "        #         cleaned += \"?\"\n",
    "        #     result[key] = cleaned\n",
    "\n",
    "        elif key == \"search string\":\n",
    "            raw = transcript.strip()\n",
    "\n",
    "            # שלב 1: מחיקה של פתיחים שמופיעים רק בתחילת המשפט\n",
    "            garbage_prefixes = [\n",
    "                \"את יכולה לבדוק לי\", \"את יכולה לבדוק\", \"את יכולה\", \"תוכל לבדוק\", \"תוכלי לבדוק\",\n",
    "                \"תגידי לי\", \"תגיד לי\", \"תגידו לי\", \"אני רוצה לדעת\", \"אני מעוניינת לדעת\",\n",
    "                \"אני מבקש לדעת\", \"אני רוצה לשאול\", \"אפשר לדעת\", \"אני מעוניין לדעת\"\n",
    "            ]\n",
    "            for phrase in sorted(garbage_prefixes, key=len, reverse=True):\n",
    "                if raw.startswith(phrase):\n",
    "                    raw = raw[len(phrase):].strip()\n",
    "\n",
    "            # שלב 2: המשך הקוד המקורי שלך — שמרנו עליו\n",
    "            garbage_phrases = [\n",
    "                \"אני רוצה\", \"אני מעוניינת\",\"מבקשת\", \"אני מבקשת\", \"הייתי רוצה\", \"בבקשה\",\n",
    "                \"תודה מראש\", \"תודה רבה\",\"להיות\", \"תודה\", \"לשאול\", \"לדעת\", \"לבדוק\",\n",
    "                \"שם\", \"מהו\", \"מה הוא\", \"מהו שם\", \"של ה\", \"תגידי\", \"תגיד\", \"תגידו\"\n",
    "            ]\n",
    "            for phrase in garbage_phrases:\n",
    "                raw = raw.replace(phrase, \"\")\n",
    "\n",
    "            garbage_words = [\"אני\", \"תודה\", \"להיות\"]\n",
    "            for word in garbage_words:\n",
    "                raw = re.sub(rf\"\\b{word}\\b\", \"\", raw)\n",
    "\n",
    "            # שלב 3: זיהוי מילת שאלה והתחלה ממנה (אופציונלי – לא אגרסיבי מדי)\n",
    "            question_starters = [\"מהי\", \"מה\", \"מי\", \"כמה\", \"מתי\", \"איך\", \"איפה\"]\n",
    "            for q_word in question_starters:\n",
    "                if q_word in raw:\n",
    "                    raw = raw[raw.index(q_word):]\n",
    "                    break\n",
    "\n",
    "            # ניקוי רווחים וסיום\n",
    "            cleaned = re.sub(r\"\\s{2,}\", \" \", raw).strip()\n",
    "            if cleaned and not cleaned.endswith(\"?\"):\n",
    "                cleaned += \"?\"\n",
    "\n",
    "            result[key] = cleaned\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "# Extract intent_json\n",
    "intent_jsons = []\n",
    "for index, row in df.iterrows():\n",
    "    transcript = row[\"command\"]\n",
    "    intent = row[\"predicted_intent\"]\n",
    "    try:\n",
    "        ner_result = ner_pipeline(transcript)\n",
    "        json_obj = build_action_json(transcript, ner_result, intent)\n",
    "        if intent == \"call_contact\":\n",
    "            contact = json_obj.get(\"contact_name\", \"\")\n",
    "            # הסר רק \"ל\" בודדת שמופיעה לפני שם, לא כחלק מהמילה\n",
    "            contact = re.sub(r\"^ל(?=\\s*[א-ת])\", \"\", contact).strip()\n",
    "            json_obj[\"contact_name\"] = contact\n",
    "\n",
    "    except Exception as e:\n",
    "        json_obj = None\n",
    "\n",
    "    intent_jsons.append(json_obj)\n",
    "\n",
    "df[\"intent_json\"] = intent_jsons\n",
    "\n",
    "# Save results\n",
    "df.to_csv(\"intent_extracted_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "# files.download(\"intent_extracted_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "b37vyGHCwts3",
    "outputId": "06da26cf-6559-4598-e087-c140ef228951"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_7b955b02-9734-4efb-b3c3-e7f12b92d66a\", \"rephrased_intents.csv\", 134027)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rephrase_from_intent_json(intent, intent_json):\n",
    "    template = intent_templates.get(intent)\n",
    "    if not template or not intent_json:\n",
    "        return None\n",
    "\n",
    "    # Rename keys if needed\n",
    "    mapping = {\n",
    "        \"search\": \"search string\"\n",
    "    }\n",
    "\n",
    "    for old_key, new_key in mapping.items():\n",
    "        if old_key in intent_json and new_key not in intent_json:\n",
    "            intent_json[new_key] = intent_json[old_key]\n",
    "\n",
    "    # החלפה של None בערכים ריקים (שיהפכו ל־\"\" במילוי התבנית)\n",
    "    filled = {\n",
    "        k: (\"\" if v is None else v)\n",
    "        for k, v in intent_json.items()\n",
    "    }\n",
    "\n",
    "    # ניקוי כפילות של period מתוך time (למשל: \"תשע ארבעים בערב\" + \"בערב\")\n",
    "    if intent == \"alarm_set\":\n",
    "        time_val = filled.get(\"time\", \"\")\n",
    "        period_val = filled.get(\"period\", \"\")\n",
    "        if time_val and period_val and period_val in time_val:\n",
    "            filled[\"period\"] = \"\"  # אל תוסיף period שוב\n",
    "\n",
    "    try:\n",
    "        rephrased = template.format(**filled).strip()\n",
    "        rephrased = re.sub(r\"\\s{2,}\", \" \", rephrased)  # הסרת רווחים כפולים\n",
    "        return rephrased\n",
    "    except Exception as e:\n",
    "        return f\"שגיאה בפורמט: {e}\"\n",
    "\n",
    "\n",
    "# Create a new column with rephrased sentence\n",
    "df[\"rephrased\"] = df.apply(lambda row: rephrase_from_intent_json(row[\"predicted_intent\"], row[\"intent_json\"]), axis=1)\n",
    "\n",
    "\n",
    "# Save the new CSV including rephrased column\n",
    "df.to_csv(\"rephrased_intents.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "files.download(\"rephrased_intents.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
